{'setup': 'simclr', 'backbone': 'resnet18', 'model_kwargs': {'head': 'mlp', 'features_dim': 128}, 'train_db_name': 'cifar-10', 'val_db_name': 'cifar-10', 'num_classes': 10, 'criterion': 'simclr', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 500, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.0001, 'momentum': 0.9, 'lr': 0.4}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 512, 'num_workers': 8, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 32, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.4914, 0.4822, 0.4465], 'std': [0.2023, 0.1994, 0.201]}}, 'transformation_kwargs': {'crop_size': 32, 'normalize': {'mean': [0.4914, 0.4822, 0.4465], 'std': [0.2023, 0.1994, 0.201]}}, 'pretext_dir': '/content/result/cifar-10/pretext', 'pretext_checkpoint': '/content/result/cifar-10/pretext/checkpoint.pth.tar', 'pretext_model': '/content/result/cifar-10/pretext/model.pth.tar', 'topk_neighbors_train_path': '/content/result/cifar-10/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': '/content/result/cifar-10/pretext/topk-val-neighbors.npy'}
Retrieve model
Model is ContrastiveModel
Model parameters: 11.50M
ContrastiveModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  )
  (contrastive_head): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=128, bias=True)
  )
)
Set CuDNN benchmark
Retrieve dataset
Train transforms: Compose(
    RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
)
Validation transforms: Compose(
    CenterCrop(size=(32, 32))
    ToTensor()
    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
)
Files already downloaded and verified
Files already downloaded and verified
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Dataset contains 50000/10000 train/val samples
Build MemoryBank
Files already downloaded and verified
Retrieve criterion
Criterion is SimCLRLoss
Retrieve optimizer
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.4
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)
No checkpoint file at /content/result/cifar-10/pretext/checkpoint.pth.tar
Starting main loop
Epoch 0/500
---------------
Adjusted learning rate to 0.40000
Train ...
Epoch: [0][ 0/97]	Loss 6.5859e+00 (6.5859e+00)
Epoch: [0][25/97]	Loss 4.6811e+00 (5.3622e+00)
Epoch: [0][50/97]	Loss 4.2786e+00 (4.9026e+00)
Epoch: [0][75/97]	Loss 4.0590e+00 (4.6526e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 25.70
Checkpoint ...
Epoch 1/500
---------------
Adjusted learning rate to 0.40000
Train ...
Epoch: [1][ 0/97]	Loss 4.1076e+00 (4.1076e+00)
Epoch: [1][25/97]	Loss 3.9281e+00 (4.0089e+00)
Epoch: [1][50/97]	Loss 3.9899e+00 (3.9816e+00)
Epoch: [1][75/97]	Loss 3.8083e+00 (3.9701e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 26.00
Checkpoint ...
Epoch 2/500
---------------
Adjusted learning rate to 0.39998
Train ...
Epoch: [2][ 0/97]	Loss 3.8912e+00 (3.8912e+00)
Epoch: [2][25/97]	Loss 3.8717e+00 (3.8693e+00)
Epoch: [2][50/97]	Loss 3.8138e+00 (3.8749e+00)
Epoch: [2][75/97]	Loss 3.7618e+00 (3.8729e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 28.18
Checkpoint ...
Epoch 3/500
---------------
Adjusted learning rate to 0.39996
Train ...
Epoch: [3][ 0/97]	Loss 3.5647e+00 (3.5647e+00)
Epoch: [3][25/97]	Loss 3.4727e+00 (3.4600e+00)
Epoch: [3][50/97]	Loss 3.2646e+00 (3.4219e+00)
Epoch: [3][75/97]	Loss 3.2728e+00 (3.3701e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 29.94
Checkpoint ...
Epoch 4/500
---------------
Adjusted learning rate to 0.39994
Train ...
Epoch: [4][ 0/97]	Loss 3.0892e+00 (3.0892e+00)
Epoch: [4][25/97]	Loss 2.9993e+00 (3.0841e+00)
Epoch: [4][50/97]	Loss 2.9461e+00 (3.0423e+00)
Epoch: [4][75/97]	Loss 2.8526e+00 (2.9721e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 33.77
Checkpoint ...
Epoch 5/500
---------------
Adjusted learning rate to 0.39990
Train ...
Epoch: [5][ 0/97]	Loss 2.6614e+00 (2.6614e+00)
Epoch: [5][25/97]	Loss 2.5355e+00 (2.5520e+00)
Epoch: [5][50/97]	Loss 2.2378e+00 (2.4732e+00)
Epoch: [5][75/97]	Loss 2.1901e+00 (2.3939e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 37.37
Checkpoint ...
Epoch 6/500
---------------
Adjusted learning rate to 0.39986
Train ...
Epoch: [6][ 0/97]	Loss 2.0126e+00 (2.0126e+00)
Epoch: [6][25/97]	Loss 1.9295e+00 (1.9714e+00)
Epoch: [6][50/97]	Loss 1.8244e+00 (1.9209e+00)
Epoch: [6][75/97]	Loss 1.6918e+00 (1.8704e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 39.67
Checkpoint ...
Epoch 7/500
---------------
Adjusted learning rate to 0.39981
Train ...
Epoch: [7][ 0/97]	Loss 1.6218e+00 (1.6218e+00)
Epoch: [7][25/97]	Loss 1.5399e+00 (1.5989e+00)
Epoch: [7][50/97]	Loss 1.5164e+00 (1.5651e+00)
Epoch: [7][75/97]	Loss 1.3164e+00 (1.5262e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 40.64
Checkpoint ...
Epoch 8/500
---------------
Adjusted learning rate to 0.39975
Train ...
Epoch: [8][ 0/97]	Loss 1.3002e+00 (1.3002e+00)
Epoch: [8][25/97]	Loss 1.3474e+00 (1.3362e+00)
Epoch: [8][50/97]	Loss 1.2123e+00 (1.3040e+00)
Epoch: [8][75/97]	Loss 1.1920e+00 (1.2672e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 42.21
Checkpoint ...
Epoch 9/500
---------------
Adjusted learning rate to 0.39968
Train ...
Epoch: [9][ 0/97]	Loss 1.1535e+00 (1.1535e+00)
Epoch: [9][25/97]	Loss 1.0878e+00 (1.1278e+00)
Epoch: [9][50/97]	Loss 9.3816e-01 (1.1007e+00)
Epoch: [9][75/97]	Loss 1.0642e+00 (1.0802e+00)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 44.14
Checkpoint ...
Epoch 10/500
---------------
Adjusted learning rate to 0.39961
Train ...
Epoch: [10][ 0/97]	Loss 9.2704e-01 (9.2704e-01)
Epoch: [10][25/97]	Loss 9.4444e-01 (9.6435e-01)
Epoch: [10][50/97]	Loss 9.7220e-01 (9.4769e-01)
Epoch: [10][75/97]	Loss 8.7955e-01 (9.4157e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 44.66
Checkpoint ...
Epoch 11/500
---------------
Adjusted learning rate to 0.39952
Train ...
Epoch: [11][ 0/97]	Loss 8.8586e-01 (8.8586e-01)
Epoch: [11][25/97]	Loss 8.1383e-01 (8.6428e-01)
Epoch: [11][50/97]	Loss 8.4677e-01 (8.5497e-01)
Epoch: [11][75/97]	Loss 7.3829e-01 (8.5068e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 44.63
Checkpoint ...
Epoch 12/500
---------------
Adjusted learning rate to 0.39943
Train ...
Epoch: [12][ 0/97]	Loss 7.9652e-01 (7.9652e-01)
Epoch: [12][25/97]	Loss 7.9360e-01 (8.0422e-01)
Epoch: [12][50/97]	Loss 7.5893e-01 (7.9643e-01)
Epoch: [12][75/97]	Loss 8.1931e-01 (7.9056e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 45.58
Checkpoint ...
Epoch 13/500
---------------
Adjusted learning rate to 0.39933
Train ...
Epoch: [13][ 0/97]	Loss 6.6392e-01 (6.6392e-01)
Epoch: [13][25/97]	Loss 6.5144e-01 (7.3406e-01)
Epoch: [13][50/97]	Loss 6.8827e-01 (7.3029e-01)
Epoch: [13][75/97]	Loss 6.9198e-01 (7.3041e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 46.12
Checkpoint ...
Epoch 14/500
---------------
Adjusted learning rate to 0.39923
Train ...
Epoch: [14][ 0/97]	Loss 6.7812e-01 (6.7812e-01)
Epoch: [14][25/97]	Loss 6.6208e-01 (6.8172e-01)
Epoch: [14][50/97]	Loss 7.2627e-01 (6.7975e-01)
Epoch: [14][75/97]	Loss 6.2148e-01 (6.7624e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 47.61
Checkpoint ...
Epoch 15/500
---------------
Adjusted learning rate to 0.39911
Train ...
Epoch: [15][ 0/97]	Loss 7.1776e-01 (7.1776e-01)
Epoch: [15][25/97]	Loss 6.3303e-01 (6.5540e-01)
Epoch: [15][50/97]	Loss 6.8940e-01 (6.5234e-01)
Epoch: [15][75/97]	Loss 6.6905e-01 (6.4705e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 48.69
Checkpoint ...
Epoch 16/500
---------------
Adjusted learning rate to 0.39899
Train ...
Epoch: [16][ 0/97]	Loss 6.2721e-01 (6.2721e-01)
Epoch: [16][25/97]	Loss 6.1687e-01 (6.1603e-01)
Epoch: [16][50/97]	Loss 6.4115e-01 (6.2437e-01)
Epoch: [16][75/97]	Loss 6.2053e-01 (6.2464e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
Result of kNN evaluation is 49.17
Checkpoint ...
Epoch 17/500
---------------
Adjusted learning rate to 0.39886
Train ...
Epoch: [17][ 0/97]	Loss 5.9288e-01 (5.9288e-01)
Epoch: [17][25/97]	Loss 6.5718e-01 (6.1800e-01)
Epoch: [17][50/97]	Loss 6.4740e-01 (6.1159e-01)
Epoch: [17][75/97]	Loss 5.7541e-01 (6.0533e-01)
Fill memory bank for kNN...
Fill Memory Bank [0/98]
Evaluate ...
